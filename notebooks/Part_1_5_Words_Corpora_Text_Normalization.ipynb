{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_1_5_Words_Corpora_Text_Normalization\n",
    "In Natural Language Processing (NLP), key techniques such as tokenization, stemming, and sentence segmentation are fundamental for transforming raw text into a structured format that can be effectively analyzed by language models. Byte-Pair Encoding (BPE) tokenization helps handle rare words and out-of-vocabulary terms by breaking text into subword units. Stemming, using algorithms like the Porter Stemmer, reduces words to their root forms, ensuring consistency across the text for better analysis. Sentence segmentation is crucial for dividing text into meaningful sentences, allowing for more accurate processing in downstream tasks. These techniques play a vital role in preparing text for various NLP tasks, ensuring that language data is in a normalized and analyzable state.\n",
    "\n",
    "### **Objectives:**\n",
    "By the end of this notebook, Parham will have a thorough understanding of tokenization and its importance in NLP, specifically learning how to implement **Byte-Pair Encoding (BPE)** to handle words and subwords. He will explore the **Porter Stemmer**, gaining insight into how it reduces words to their base forms and why this is essential for text normalization. Additionally, Parham will learn to apply **Sentence Segmentation** to split text into meaningful sentences for deeper analysis. Through hands-on coding exercises, he will gain practical experience using these techniques, utilizing Python libraries like `NLTK` and `SpaCy` to prepare text for NLP tasks.\n",
    "\n",
    "**Table of Contents:** \n",
    "1. Import Libraries\n",
    "2. Tokenization Techniques\n",
    "3. Text Normalization: Porter Stemmer\n",
    "4. Sentence Segmentation\n",
    "5. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from loguru import logger\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Techniques\n",
    "Byte-pair encoding was first introduced in 1994 as a simple data compression technique by iteratively replacing the most frequent pair of bytes in a sequence with a single, unused byte.\n",
    "Imagine youâ€™re reading a really big book, but some of the words are really long or tricky, and you might not know all of them. To make things easier, Parham can break the long words into smaller, simpler parts or pieces. This way, Parham can still understand the book without needing to know every single big word.\n",
    "\n",
    "Byte-Pair Encoding (BPE) is a popular technique used in natural language processing for tokenizing text into subword units. The idea is to break down words into smaller, more frequent pieces, allowing models to efficiently handle rare or unknown words. BPE is especially useful in scenarios where the vocabulary is limited but the text contains a large variety of words, including compound or out-of-vocabulary words.\n",
    "\n",
    "The algorithm works by:\n",
    "- Starting with a sequence of individual characters.\n",
    "- Finding the most frequent pair of consecutive characters (or subwords).\n",
    "- Merging this pair into a single token.\n",
    "- Repeating the process until a predefined number of merges or a desired vocabulary size is reached.\n",
    "\n",
    "This process allows models to represent both common words and subwords effectively, making it easier to process any text.\n",
    "\n",
    "In the cell below Parham will an example of BEP tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary: defaultdict(<class 'int'>, {'l o w </w>': 1, 'l o w e s t </w>': 1, 'n e w e r </w>': 1, 'w i d e r </w>': 1})\n",
      "\n",
      "After merge 1:\n",
      "Best Pair: ('l', 'o')\n",
      "Updated Vocabulary: {'lo w </w>': 1, 'lo w e s t </w>': 1, 'n e w e r </w>': 1, 'w i d e r </w>': 1}\n",
      "\n",
      "After merge 2:\n",
      "Best Pair: ('lo', 'w')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low e s t </w>': 1, 'n e w e r </w>': 1, 'w i d e r </w>': 1}\n",
      "\n",
      "After merge 3:\n",
      "Best Pair: ('e', 'r')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low e s t </w>': 1, 'n e w er </w>': 1, 'w i d er </w>': 1}\n",
      "\n",
      "After merge 4:\n",
      "Best Pair: ('er', '</w>')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low e s t </w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 5:\n",
      "Best Pair: ('low', '</w>')\n",
      "Updated Vocabulary: {'low</w>': 1, 'low e s t </w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 6:\n",
      "Best Pair: ('low', 'e')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowe s t </w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 7:\n",
      "Best Pair: ('lowe', 's')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowes t </w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 8:\n",
      "Best Pair: ('lowes', 't')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowest </w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 9:\n",
      "Best Pair: ('lowest', '</w>')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowest</w>': 1, 'n e w er</w>': 1, 'w i d er</w>': 1}\n",
      "\n",
      "After merge 10:\n",
      "Best Pair: ('n', 'e')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowest</w>': 1, 'ne w er</w>': 1, 'w i d er</w>': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'low</w>': 1, 'lowest</w>': 1, 'ne w er</w>': 1, 'w i d er</w>': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Corpus (small set of words)\n",
    "corpus = [\"low\", \"lowest\", \"newer\", \"wider\"]\n",
    "\n",
    "# Create initial vocabulary from characters in the corpus\n",
    "def get_vocab(corpus):\n",
    "    vocab = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        # Add a space between each character, so we can merge pairs\n",
    "        word = \" \".join(list(word)) + \" </w>\"\n",
    "        vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "# Get the most frequent pair of symbols in the vocabulary\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "# Merge the most frequent pair in the vocabulary\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        # Replace the pair with the merged version\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        v_out[new_word] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# Perform Byte-Pair Encoding\n",
    "def byte_pair_encoding(corpus, num_merges):\n",
    "    vocab = get_vocab(corpus)\n",
    "    print(\"Initial Vocabulary:\", vocab)\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print(f\"\\nAfter merge {i + 1}:\")\n",
    "        print(f\"Best Pair: {best_pair}\")\n",
    "        print(f\"Updated Vocabulary: {vocab}\")\n",
    "        \n",
    "    return vocab\n",
    "\n",
    "# Run BPE with 10 merges on the example corpus\n",
    "byte_pair_encoding(corpus, num_merges=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Normalization: Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Closing Thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
