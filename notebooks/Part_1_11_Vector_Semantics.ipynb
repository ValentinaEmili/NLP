{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.di.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png' width=\"200\"/>  \n",
    "\n",
    "# Part_1_11_Vector Semantics (Sparse)\n",
    "\n",
    "In Natural Language Processing (`NLP`), vector semantics provides a powerful framework for representing words and documents in a numerical form, enabling efficient computation and semantic analysis. Sparse vector representations, such as **Bag of Words (`BoW`)**, **TF-IDF**, and **Pointwise Mutual Information (`PPMI`)**, have been foundational in the evolution of `NLP`. These approaches rely on statistical co-occurrence patterns and word frequency to capture linguistic meaning, forming the basis for more sophisticated methods like dense embeddings and contextualized models.\n",
    "\n",
    "Sparse representations are particularly useful in understanding the core principles of vector semantics and building intuition about the role of word-document relationships in tasks like text classification, clustering, and retrieval systems.\n",
    "\n",
    "### **Objectives:**\n",
    "In this notebook, Parham provides an overview of sparse vector semantics, including the key methods used to represent text data and their significance in `NLP`. Through practical exercises, Parham will demonstrate the implementation of **Bag of Words (`BoW`)** for document representation, **TF-IDF** to highlight significant terms within documents, and **PPMI** to extract meaningful statistical relationships from co-occurrence matrices.\n",
    "\n",
    "### **References:**\n",
    "- [https://www.datacamp.com/tutorial/python-bag-of-words-model](https://www.datacamp.com/tutorial/python-bag-of-words-model)  \n",
    "- [https://spotintelligence.com/2022/12/20/bag-of-words-python](https://spotintelligence.com/2022/12/20/bag-of-words-python/)  \n",
    "- [https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus](https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus)   \n",
    "\n",
    "### **Tutors**:\n",
    "- Professor Stefano Farali\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: Stefano.faralli@uniroma1.it\n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/stefano-faralli-b1183920/) \n",
    "- Professor Iacopo Masi\n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: masi@di.uniroma1.it  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/iacopomasi/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/iacopomasi)  \n",
    "    \n",
    "\n",
    "### **Contributors:**\n",
    "- Parham Membari  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Gmail_icon_%282020%29.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Email**: p.membari96@gmail.com  \n",
    "    - <img src=\"https://www.iconsdb.com/icons/preview/red/linkedin-6-xxl.png\" alt=\"Logo\" width=\"20\" height=\"20\"> **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/p-mem/)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Github-desktop-logo-symbol.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **GitHub**: [GitHub](https://github.com/parham075)  \n",
    "    - <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Medium_logo_Monogram.svg\" alt=\"Logo\" width=\"20\" height=\"20\"> **Medium**: [Medium](https://medium.com/@p.membari96)  \n",
    "\n",
    "**Table of Contents:**\n",
    "1. Import Libraries  \n",
    "2. Introduction to Vector Semantics  \n",
    "3. Bag of Words (`BoW`) Representation      \n",
    "4. Term Frequency-Inverse Document Frequency (`TF-IDF`)\n",
    "5. Pointwise Mutual Information (`PMI`)   \n",
    "6. Closing Thoughts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/p/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/p/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tag import pos_tag\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Vector Semantics\n",
    "Vector semantics is a methodology in `NLP` that uses numerical representations to encode the meaning of words, phrases, or entire documents. These numerical representations are essential for enabling machines to process and analyze human language. By transforming textual data into vectors, computational models can perform mathematical operations to assess similarity, context, and relationships between words or documents.\n",
    "\n",
    "### Why Vector Semantics?\n",
    "- **Quantitative Representation**: Text data, being inherently qualitative, is challenging for computers to process directly. Vector semantics bridges this gap by converting text into a mathematical form.\n",
    "- **Efficient Computation**: Mathematical representations allow for quick calculations of similarity, clustering, and classification tasks.\n",
    "- **Foundation for Machine Learning Models**: Many machine learning models rely on vectorized representations of data as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bag of Words (`BoW`) Representation \n",
    "\n",
    "**Bag of Words** (`BoW`) is a simple and widely-used representation of text data in Natural Language Processing. It represents text as a collection of words, ignoring grammar, word order, and context, while preserving the frequency of words.BoW boadly used in tasks such as text classification and sentiment analysis. This is important because machine learning algorithms can’t process textual data. The process of converting the text to numbers is known as feature extraction or feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding Bag of Words with example:\n",
    "Imagine two sentences:\n",
    "1. Document 1: \"Natural Language Processing is amazing.\"\n",
    "2. Document 2: \"Language models are important for NLP.\"\n",
    "\n",
    "\n",
    "The `BoW` model begins by creating a vocabulary, a unique list of all words across the corpus. Each document is then represented as a vector of word frequencies. Table below, represents the Bag of Words vectors:\n",
    "\n",
    "| **Vocabulary** | **Document 1** | **Document 2** |\n",
    "|-----------------|----------------|----------------|\n",
    "| Natural         | 1              | 0              |\n",
    "| Language        | 1              | 1              |\n",
    "| Processing      | 1              | 0              |\n",
    "| is              | 1              | 0              |\n",
    "| amazing         | 1              | 0              |\n",
    "| models          | 0              | 1              |\n",
    "| are             | 0              | 1              |\n",
    "| important       | 0              | 1              |\n",
    "| for             | 0              | 1              |\n",
    "| NLP             | 0              | 1              |\n",
    "\n",
    "Each position in the vector corresponds to a word in the vocabulary, and the value represents its frequency in the document.\n",
    "\n",
    "### How to implement `BoW`\n",
    "The steps involved to create `BoW` are:\n",
    "- Tokenization: Split the text into individual words or tokens.\n",
    "- Preprocessing:\n",
    "    - Convert text to lowercase.\n",
    "    - Remove special characters, punctuation, and numbers.\n",
    "    - Remove stopwords (e.g., \"the\", \"is\", \"and\").\n",
    "- Apply stemming or lemmatization to normalize words.\n",
    "- Vocabulary Creation: Build a unique list of words from the corpus.\n",
    "- Vectorization: Represent each document as a vector of word frequencies based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document 1</th>\n",
       "      <th>Document 2</th>\n",
       "      <th>Document 3</th>\n",
       "      <th>Document 4</th>\n",
       "      <th>Document 5</th>\n",
       "      <th>Document 6</th>\n",
       "      <th>Document 7</th>\n",
       "      <th>Document 8</th>\n",
       "      <th>Document 9</th>\n",
       "      <th>Document 10</th>\n",
       "      <th>Document 11</th>\n",
       "      <th>Document 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allow</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anoth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>around</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unstructur</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
       "ad                   0           0           0           0           0   \n",
       "allow                0           1           0           0           0   \n",
       "analyz               0           0           1           1           0   \n",
       "anoth                0           0           0           0           0   \n",
       "around               0           0           0           0           0   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "type                 0           0           0           0           1   \n",
       "unit                 0           0           0           0           0   \n",
       "unstructur           0           0           1           0           0   \n",
       "word                 1           0           0           1           2   \n",
       "work                 0           1           0           0           0   \n",
       "\n",
       "            Document 6  Document 7  Document 8  Document 9  Document 10  \\\n",
       "ad                   0           0           1           0            0   \n",
       "allow                0           1           0           0            0   \n",
       "analyz               0           0           1           0            1   \n",
       "anoth                0           0           0           0            1   \n",
       "around               0           0           0           0            0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "type                 0           0           0           0            0   \n",
       "unit                 1           0           0           0            0   \n",
       "unstructur           0           0           0           0            0   \n",
       "word                 0           2           1           0            1   \n",
       "work                 0           0           0           0            0   \n",
       "\n",
       "            Document 11  Document 12  \n",
       "ad                    0            0  \n",
       "allow                 0            0  \n",
       "analyz                0            0  \n",
       "anoth                 0            0  \n",
       "around                1            0  \n",
       "...                 ...          ...  \n",
       "type                  0            0  \n",
       "unit                  0            0  \n",
       "unstructur            0            0  \n",
       "word                  2            0  \n",
       "work                  0            0  \n",
       "\n",
       "[78 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "paragraph = \"\"\"\n",
    "By tokenizing, you can conveniently split up text by word or by sentence. \n",
    "This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. \n",
    "It’s your first step in turning unstructured data into structured data, which is easier to analyze.\n",
    "\n",
    "When you’re analyzing text, you’ll be tokenizing by word and tokenizing by sentence. \n",
    "Here’s what both types of tokenization bring to the table:\n",
    "\n",
    "Tokenizing by word: Words are like the atoms of natural language. \n",
    "They’re the smallest unit of meaning that still makes sense on its own. \n",
    "Tokenizing your text by word allows you to identify words that come up particularly often. \n",
    "For example, if you were analyzing a group of job ads, \n",
    "then you might find that the word “Python” comes up often. \n",
    "That could suggest high demand for Python knowledge, but you’d need to look deeper to know more.\n",
    "\n",
    "Tokenizing by sentence: When you tokenize by sentence, \n",
    "you can analyze how those words relate to one another and see more context. \n",
    "Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? \n",
    "Are there more terms from the domain of herpetology than the domain of software development, \n",
    "suggesting that you may be dealing with an entirely different kind of python than you were expecting?\n",
    "\n",
    "\"\"\"\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Step 2: Preprocessing each sentence\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Remove special characters, numbers, and punctuations\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Convert to lowercase\n",
    "    review = review.lower()\n",
    "    # Split into words\n",
    "    review = review.split()\n",
    "    # Remove stopwords and apply stemming\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    # Join the words back into a sentence\n",
    "    review = ' '.join(review)\n",
    "    # Add the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Step 3: Create Bag of Words model\n",
    "cv = CountVectorizer(max_features=1500)  # Limit to top 1500 features (if necessary)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "vocabulary = cv.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame with each sentence as a column\n",
    "bow_df = pd.DataFrame(X.T, index=vocabulary, columns=[f\"Document {i+1}\" for i in range(X.shape[0])])\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Term Frequency-Inverse Document Frequency (`TF-IDF`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pointwise Mutual Information (`PMI`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Closing Thoughts "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
