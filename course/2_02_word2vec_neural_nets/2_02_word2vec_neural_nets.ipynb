{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### 2.2 Scaling word2vec and introduction to Neural Nets for NLP\n",
    "<br><br>\n",
    "Prof. Iacopo Masi and Prof. Stefano Faralli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.colheader_justify', 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# Aux functions\n",
    "\n",
    "def plot_grid(Xs, Ys, axs=None):\n",
    "    ''' Aux function to plot a grid'''\n",
    "    t = np.arange(Xs.size) # define progression of int for indexing colormap\n",
    "    if axs:\n",
    "        axs.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        axs.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        axs.axis('scaled') # axis scaled\n",
    "    else:\n",
    "        plt.plot(0, 0, marker='*', color='r', linestyle='none') #plot origin\n",
    "        plt.scatter(Xs,Ys, c=t, cmap='jet', marker='.') # scatter x vs y\n",
    "        plt.axis('scaled') # axis scaled\n",
    "        \n",
    "def linear_map(A, Xs, Ys):\n",
    "    '''Map src points with A'''\n",
    "    # [NxN,NxN] -> NxNx2 # add 3-rd axis, like adding another layer\n",
    "    src = np.stack((Xs,Ys), axis=Xs.ndim)\n",
    "    # flatten first two dimension\n",
    "    # (NN)x2\n",
    "    src_r = src.reshape(-1,src.shape[-1]) #ask reshape to keep last dimension and adjust the rest\n",
    "    # 2x2 @ 2x(NN)\n",
    "    dst = A @ src_r.T # 2xNN\n",
    "    #(NN)x2 and then reshape as NxNx2\n",
    "    dst = (dst.T).reshape(src.shape)\n",
    "    # Access X and Y\n",
    "    return dst[...,0], dst[...,1]\n",
    "\n",
    "\n",
    "def plot_points(ax, Xs, Ys, col='red', unit=None, linestyle='solid'):\n",
    "    '''Plots points'''\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, which='both')\n",
    "    ax.axhline(y=0, color='gray', linestyle=\"--\")\n",
    "    ax.axvline(x=0, color='gray',  linestyle=\"--\")\n",
    "    ax.plot(Xs, Ys, color=col)\n",
    "    if unit is None:\n",
    "        plotVectors(ax, [[0,1],[1,0]], ['gray']*2, alpha=1, linestyle=linestyle)\n",
    "    else:\n",
    "        plotVectors(ax, unit, [col]*2, alpha=1, linestyle=linestyle)\n",
    "\n",
    "def plotVectors(ax, vecs, cols, alpha=1, linestyle='solid'):\n",
    "    '''Plot set of vectors.'''\n",
    "    for i in range(len(vecs)):\n",
    "        x = np.concatenate([[0,0], vecs[i]])\n",
    "        ax.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha, linestyle=linestyle, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## My own latex definitions\n",
    "\n",
    "$$\\def\\mbf#1{\\mathbf{#1}}$$\n",
    "$$\\def\\bmf#1{\\boldsymbol{#1}}$$\n",
    "$$\\def\\bx{\\mbf{x}}$$\n",
    "$$\\def\\bxt#1{\\mbf{x}_{\\text{#1}}}$$\n",
    "$$\\def\\bv{\\mbf{v}}$$\n",
    "$$\\def\\bz{\\mbf{z}}$$\n",
    "$$\\def\\bmu{\\bmf{\\mu}}$$\n",
    "$$\\def\\bsigma{\\bmf{\\Sigma}}$$\n",
    "$$\\def\\Rd#1{\\in \\mathbb{R}^{#1}}$$\n",
    "$$\\def\\chain#1#2{\\frac{\\partial #1}{\\partial #2}}$$\n",
    "$$\\def\\loss{\\mathcal{L}}$$\n",
    "$$\\def\\params{\\bmf{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "## - Skip-Gram and Continuous Bag of Words (CBOW)\n",
    "## - How to scale word2vec\n",
    "### - Negative Sampling\n",
    "### - Hierarchical Softmax\n",
    "## - Introduction to Neural Nets for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This lecture material is taken from\n",
    "üìò **Chapter 6 Jurafsky Book**\n",
    "\n",
    "üìò **Chapter 14.5 Eisenstein Book**\n",
    "- [Stanford Slide Word2Vec](http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture01-wordvecs2.pdf)\n",
    "- [Stanford Lecture Word2Vec](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=1)\n",
    "- [Stanford Notes on Word2Vec](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)\n",
    "\n",
    "\n",
    "üìù Research papers on word2vec and hierarchical softmax:\n",
    "- [First paper: word2vec + hierarchical softmax](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Negative Sampling paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- [A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems](http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec is a generic framework \n",
    "\n",
    "`word2vec` presents two algorithms:\n",
    " 1. ~~Skip-Gram~~\n",
    " 2. **Continuous Bag-of-Word** (CBOW)  _(we see it today!)_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "\n",
    "Also it offers different training methods:\n",
    " - ~~with softmax~~  \n",
    " - **negative sampling** from [Mikolov et al. 2013]\n",
    " - **hierarchical softmax** _(we see them today!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec: Skip-Gram Self-Supervision\n",
    "\n",
    "‚ö†Ô∏è With **strong naive conditional independence assumption** \n",
    "<br/>\n",
    "\n",
    "$$ p(w_{t-2},w_{t-1},w_{t+1},w_{t+2}|w_t;\\bmf{\\theta}) \\approx \\prod_{-m\\leq j\\leq m}^m p(c_{t+j}|w_t;\\bmf{\\theta})$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "|   | ~~lemon~~, | ~~a~~ | [tablespoon | of | apricot | jam | a] | ~~pinch~~ |\n",
    "|:-:|:------:|:-:|:-----------:|:--:|:-------:|:---:|:--:|:-----:|\n",
    "|   |        |   |      w_{t-2}     | w_{t-1} |    **$w_t$**    |  w_{t+1} | w_{t+2} |       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec: Skip-Gram Self-Supervision\n",
    "\n",
    "Given the example below, we have to compute:\n",
    "<br/>\n",
    "\n",
    "$$p(w_{t-2}|w_t)\\cdot p(w_{t-1}|w_t)\\cdot p(w_{t+1}|w_t)\\cdot p(w_{t+2}|w_t)$$\n",
    "<br/><br/>\n",
    "<div align='center'><img src=\"figs/word2vec_skipgram.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec: Skip-Gram with softmax\n",
    "\n",
    "Parameters to learn: $$\\bmf{\\theta} = [\\bmf{\\theta}_W;\\bmf{\\theta}_C]$$\n",
    "\n",
    "<div align='center'><img src=\"figs/word2vec_params.png\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec with Skip-Gram at a glance\n",
    "\n",
    "... and why it can be seen as a tiny neural net.\n",
    "\n",
    "<div align='center'><img src=\"figs/word2vec_layers.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function: compare two discrete distributions\n",
    " \n",
    "$$\\mbf{p}=\\operatorname{softmax}\\big(\\bmf{\\theta}_C\\cdot\\bmf{\\theta}_{W}[i]^T\\big) $$\n",
    "\n",
    "You can think $\\mbf{p}$ as of this form:\n",
    "\n",
    "|    \t| lemon   \t| tablespoon \t| gelato             \t| ...\t | jam\t|\n",
    "|----------|----------\t|--------\t|--------------------\t|--------\t|--------\t|\n",
    "|**p (word2vec prediction)**        | 0.001 | 0.1 | 0.03  | ... | 0.15 |\n",
    "\n",
    "\n",
    "Let's consider the label as a one-hot encoding vector where 1 is over the ground-truth word given by the text.\n",
    "\n",
    "|    \t| lemon   \t| tablespoon \t| gelato             \t| ...\t | jam\t|\n",
    "|----------|----------\t|--------\t|--------------------\t|--------\t|--------\t|\n",
    "|**y label (ground-truth $w_{t-1}$)** | 0 | 1 | 0  | ... | 0 |\n",
    "\n",
    "### We want to adjust the weights $\\bmf{\\theta}$ so that $\\mbf{p}$ matches the label!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-hot encoding is a selector!\n",
    "\n",
    "\n",
    "- $\\mbf{y}=[0,0,0,1,0]$ $\\rightarrow$ works as a selector of probability of the actual ground-truth word that we removed!\n",
    "- Of the probabilites returned by `word2vec` select that for which the index $gt$ corresponds to the `1` in the label $\\mbf{y}$\n",
    "- in our case, $gt$ is the index of `tablespoon` in $|V|$.\n",
    "\n",
    "$$ \\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = -\\log \\big[\\mbf{p}\\big(w_{t-1}|w_{t}\\big)\\big]{[gt]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function simplified\n",
    "\n",
    "We can select immediately $[gt]$ in the numerator.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = -\\log \\left(\\frac{\\exp \\big( \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T \\big)}{\\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)}\\right)\n",
    "$$\n",
    "\n",
    "Sometimes is shown as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = \\underbrace{-\\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T}_{\\text{similarity center vs context}} + \\underbrace{\\log\\Big(\\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)\\Big)}_{\\text{make sure it is a probability}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# word2vec: Continuous Bag-of-Word (C-BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-Gram\n",
    "\n",
    "$$ p(w_{t-2},w_{t-1},w_{t+1},w_{t+2}|w_t;\\bmf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# C-BOW\n",
    "\n",
    "$$ p(w_t|w_{t-2},w_{t-1},w_{t+1},w_{t+2};\\bmf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec: C-BOW\n",
    "\n",
    "Given the context $w_{t-2},w_{t-1},w_{t+1},w_{t+2} \\longrightarrow p(w_t)$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "|   | ~~lemon~~, | ~~a~~ | [tablespoon | of | ----?---| jam | a] | ~~pinch~~ |\n",
    "|:-:|:------:|:-:|:-----------:|:--:|:-------:|:---:|:--:|:-----:|\n",
    "|   |        |   |      w_{t-2}     | w_{t-1} |    **$w_t$**    |  w_{t+1} | w_{t+2} |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# word2vec: Continuous Bag-of-Word (CBOW)\n",
    "\n",
    "$$w_{t-2},w_{t-1},w_{t+1},w_{t+2} \\longrightarrow p(w_t)$$<br>\n",
    " \n",
    "$$\\bmf{\\theta}_{C_{avg}} = \\sum_{-m\\leq j\\leq m,~j \\neq 0} \\bmf{\\theta}_{C}[t+j]$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/word2vec_cbow.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec: Continuous Bag-of-Word (CBOW)\n",
    "\n",
    "<div align='center'><img src=\"figs/word2vec_layers_cbow.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to compute $p(w_{t-1}|w_{avg})$?\n",
    "1. $\\bmf{\\theta}_{C_{avg}}$ is the average embedding from the context parameters $\\bmf{\\theta}_{C}$. The average is computed once defined the context window size.\n",
    "\n",
    "$$\\underbrace{\\mbf{z}}_{|V|\\times 1}=\\overbrace{\\underbrace{\\bmf{\\theta}_C}_{|V|\\times D}}^{\\text{as center}}\\cdot\\overbrace{\\underbrace{\\bmf{\\theta}_{C_{avg}}^T}_{D\\times 1}}^{\\text{as context}} $$\n",
    "\n",
    "2. $\\mbf{z}$ is logits and encodes the similarity via dot product of the average context word embedding $\\bmf{\\theta}_{C_{avg}}$ **against all vocabulary words** taken as center $\\bmf{\\theta}_C$\n",
    "\n",
    "3. We pass $\\mbf{z}$ through softmax operator to get a distribution over $|V|$ as: $$\\mbf{p}=\\operatorname{softmax}(\\mbf{z}) $$\n",
    "\n",
    "You can think $\\mbf{p}$ as of this form:\n",
    "\n",
    "|    \t| lemon   \t| tablespoon \t| gelato             \t| ...\t | jam\t|\n",
    "|----------|----------\t|--------\t|--------------------\t|--------\t|--------\t|\n",
    "|**p** | 0.001 | 0.1 | 0.03  | ... | 0.15 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipgram vs CBOW at a glance\n",
    "\n",
    "<div align='center'><img src=\"figs/skip_vs_cbow.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computational Complexity\n",
    "\n",
    "- word2vec computes a **normalized probability over\n",
    "word tokens (over the vocabulary) $V$**\n",
    "- A naive  implementation of this probability requires <ins>summing over the entire vocabulary $V$</ins>\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = -\\log \\left(\\frac{\\exp \\big( \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T \\big)}{\\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)}\\right)\n",
    "$$\n",
    "\n",
    "The normalization in the denominator is of the order of $$ \\mathcal{O}(|V|\\times D)$$ where:\n",
    "- $|V|$ is the vocabulary size and e.g. $|V|=3M$ \n",
    "- $D$ is the dimension of the embeddings e.g. $D=300$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Two solutions to approximate the denominator\n",
    "\n",
    "\n",
    "1. Negative sampling (Contrastive method)\n",
    "2. Hierarchical Softmax (Tree-based solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling word2vec with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-gram with Negative Sampling (SGNS)\n",
    "\n",
    "Instead of doing:\n",
    "\n",
    "1. **Center word vs ground-truth context embedding** $\\longrightarrow \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T$\n",
    "2. normalize as a distribution: **all context vs center word** $\\longrightarrow \\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w_{t-1},w_{t};\\mbf{\\theta}) = \\underbrace{-\\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T}_{\\text{similarity center vs context}} + \\underbrace{\\log\\Big(\\sum_{v=1}^{V} \\exp \\big(\\bmf{\\theta}_{C}[v]\\cdot\\bmf{\\theta}_{W}[i]^T\\big)\\Big)}_{\\text{make sure it is a probability}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-gram with Negative Sampling (SGNS)\n",
    "\n",
    "We \"relax\" the denominator and do:\n",
    "\n",
    "1. _[Same as before]_ **Center word vs ground-truth context embedding** $\\longrightarrow \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T$\n",
    "2. Approximate the denominator as:\n",
    " - Sample a $k$ context words from $V$.\n",
    " - Compare **each $k$-th sampled word vs center word**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Negative Sampling is a form of Contrastive Method\n",
    "\n",
    "<br><div align='center'><img src=\"figs/word2vec_contrastive.png?1\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Negative Sampling is a form of Contrastive Method\n",
    "\n",
    "- **Positive Constraint:**\n",
    "    - Pull $\\bmf{\\theta}_{C}[gt]$ and $\\bmf{\\theta}_{W}$ to be \"close\" in space (give high dot-product score)\n",
    "    - Center and ground-truth context need to have high similarity\n",
    "    - In doing so we may make other words unrelated to be close in the space, so **we have to push them away**\n",
    "- **Negative Constraint:**\n",
    "    - We have to push away $\\bmf{\\theta}_{W}$ for any other remaining $\\bmf{\\theta}_{C}[i] \\quad \\forall i \\in V, i \\neq gt$ \n",
    "    - Before, this was done by forcing the total \"mass\" over the vocabulary distribution to sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Consider the simplified case: a center vs a context word\n",
    "\n",
    "<br><div align='center'><img src=\"figs/word2vec_negative_onesample.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-gram with Negative Sampling (SGNS)\n",
    "\n",
    "- Given $w_t$ the center word and $w_{t-2}$ the context word to be predicted; \n",
    "- Assuming $gt$ is the index of $w_{t-2}$ in $V$ and $i$ is the index of $w_t$ in $V$: \n",
    "<br><br>\n",
    "1. **[Positive] Center word vs ground-truth context embedding** $\\longrightarrow \\bmf{\\theta}_{C}[gt]\\cdot\\bmf{\\theta}_{W}[i]^T$\n",
    "2. **[Negative]** Sample $k$ negative indexes $v$ from $V$ such that: $v \\in V, v \\neq gt$\n",
    "\n",
    "$$ \\underbrace{1}_{\\text{positive; center vs context}} ~~~\\text{vs}~~~ \\underbrace{k}_{\\text{negative; center vs non-context}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Skip-gram with Negative Sampling (SGNS) over a window\n",
    "- Window size is $m=2$ ($m$ is one side of the window)\n",
    "- Number of negative $k=2$\n",
    "\n",
    "|   | ~~lemon~~, | ~~a~~ | [tablespoon | of | apricot| jam | a] | ~~pinch~~ |\n",
    "|:-:|:------:|:-:|:-----------:|:--:|:-------:|:---:|:--:|:-----:|\n",
    "|   |        |   |      w_{t-2}     | w_{t-1} |    **$w_t$**    |  w_{t+1} | w_{t+2} |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/positive.png\" width='35%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<div align='center'><img src=\"figs/negative.png\" width='55%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# so, how do we model that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From global softmax to binary classification for each pair\n",
    "\n",
    "Recall: words are assumed all conditionally independent [it is not true in pratice!]\n",
    "\n",
    "1. We instantiate a **binary classifier as Logistic Regression** (same as softmax but just two classes, either positive pair $y=+1$ or negative pair $y=0$)\n",
    "2. We use the sigmoid function $\\bmf{\\sigma}$ to map the dot-product result to a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "\n",
    "$$f_{\\boldsymbol{\\theta}}(\\mbf{x}) \\doteq \\sigma\\left(  \\bmf{\\theta}^T\\mbf{x} \\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\sigma(z)= \\frac{1}{1+\\exp^{-z}} \\quad \\text{sigmoid or logistic function}$$\n",
    "\n",
    "Other important properties:\n",
    "\n",
    "$$\\sigma(z)= \\frac{\\exp^{z}}{1+\\exp^{z}}; \\quad \\sigma(z)= 1-\\sigma(-z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    },
    "variables": {
     "import numpy as np;import matplotlib.pyplot as plt;x = np.arange(-20.0, 20.0, 0.1);y = 1/(1+np.exp(-x));_=plt.plot(x,y);": "<img src=\"data:image/png;base64,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\n\"/>"
    }
   },
   "source": [
    "<br><br> <center>Smooth and Differentiable alternative to sign</center>\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAiIAAAGfCAYAAABiCLkcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0nUlEQVR4nO3de3xU1b3///fM5DJJyG0gCYFc5FoQKxE9qEApUrBfoJ7fQRS12Bat4rEXi6A/DK1F5efBY6s9PW0954hKFS1SKWgV/SKtV2g9thosYlAQDJGEhEvIhNxmMrN/f0wyMiSBCZlkz+x5PR+PecCs2TPz2W5n5s1aa69tMwzDEAAAgAnsZhcAAADiF0EEAACYhiACAABMQxABAACmIYgAAADTEEQAAIBpCCIAAMA0BBEAAGCaBLMLOBO/36+qqiqlp6fLZrOZXQ4AAAiDYRhqaGjQkCFDZLd33+8R9UGkqqpKhYWFZpcBAADOQmVlpQoKCrp9POqDSHp6uqTAjmRkZJhcDQAACIfb7VZhYWHwd7w7UR9EOoZjMjIyCCIAAMSYM02rYLIqAAAwDUEEAACYhiACAABMQxABAACmIYgAAADTEEQAAIBpCCIAAMA0BBEAAGCaXgWR7du3n3GbF154Qddff73uu+8+XXPNNfr4449785YAAMBCzmpl1W3btumuu+7S559/rs8+++y02910003as2ePsrKytH37ds2YMUO7du1ilVQAANDzHpETJ06osLBQ55133hm3veeee3TFFVcoKytLkjR58mSlpKRo9erVPS4UAABYT4+DyIABA1RcXKzBgwefdru6ujq9+eabuuCCC0Lax48fr/Xr1/f0bQEAgAX12UXvdu7cqba2NhUWFoa0FxQU6IUXXpBhGF1eCKe1tVWtra3B+263u69KBICoZxiGWtv8avb41Oxtv3l8avH61OL1q83vV5vPUJvfUJvfL5/faL/vD7S1P+bz++U3JMOQDBkyjMDr+/2GDIW2G4E3liHJb3zR1rGNgve/eJ2I73ffvGyf1Su1/7eJUUtmjla6M9GU9+6zIFJTUyNJSktLC2lPSUmR1+vV0aNHNWjQoE7PW7Vqle69996+KgsATOX3GzrS2Krq4y2qrm/RofpmVbtbdLzRq+PNHh1v8qq+2Rv8s6XN16c/noAk3TpthPWCiNfrlSQ5HI7QN0w4/VuWlpZqyZIlwftut7tTrwoAxIKjJ1q1o/K4Pq5p0N6aE9p7+IT21p5Qk8d3Vq+X5LDLmWhXSpJDqUkJSk6wK8Fhk8NuV4LdFri130+02+Rov59gt8tht8lmk2wK/Gk/6e+Bzuku2vXFJdzttpPbAu3tT1P73yLuDFePP/vX7ZuXDbx2X754H0pN6rM4cEZ99s45OTmSpKamppD2lpYWJSYmKjs7u8vnJScnKzk5ua/KAoA+U9/s1Rsf12rbniN6r6JO+440drmd3Sblpjs1ONOp/Eyn8jKcGjQgSZmpScpKSVR2apKyUhOV4UxUSpJDKUkOORPsSnCw9BOsp8+CyPnnny+bzaaqqqqQ9traWk2YMKFTTwkAxKL6Zq9e/KBK//fDQ3pn31G1+UPHUUbmDtC4IRkalTtAI3PTNSpvgIpcqUokVACS+jCI5OXlacqUKdqxY0dIe3l5ua688sq+elsA6BcfVbn12LZ92vyParW2+YPto3IHaPrYXF08zKUJRdnKSk0ysUog+p11EPH5fDK6mEG1aNEiHTx4UJs3b9YDDzygq6++Wg888IAyMjJUUVGh2tpa3XLLLb0qGgDM8uHBej289RO9trs22PalvHRdOWGoLh83WMMGpZ3m2QBO1eMg4vF49Oqrr+qll15SdXW1nnzySU2bNk3FxcWSAuuH1NYGPqCTJk3SI488okWLFqmkpEQVFRV6+eWXgwucAUCsOHKiVT/f8rHW/71ShhGY5zH7y/m6ccowXVCY1eVyBADOzGZ01a0RRdxutzIzM1VfX8+y8ABM8X8/rNZdG3fqeFPgbMB/Hj9Et88cTe8HcBrh/n6bd74OAES5Fq9PP33hQ/3+759LksbmZ2jl/zNOF53jMrkywDoIIgDQhcMNrbr5qb9rR+Vx2WzSrV8docUzRispgbNdgEgiiADAKfbWntB3nnhXB483KzMlUf+1YIImjey8EjSA3iOIAMBJ9h9p1DdXv6PahlYNG5Smx79zkYbnDDC7LMCyCCIA0O7A0SZd92gghIwZnK7f3XyJXGmsAwL0JQY7AUCBFVIXrnlXh9wtGpU7QE/fdDEhBOgHBBEAcc/nN3TbujLtO9KooVkpeubmizVoANe8AvoDQQRA3Htwy269+clhORPt+p9vXajcdKfZJQFxgyACIK5t33tE//PmPknSz64ar/OGZppcERBfCCIA4pa7xav/d8M/JEkLLi7SFeOHmFwREH8IIgDi1v/30kc6eLxZRa5ULZ891uxygLhEEAEQl/6y94h+//fPZbNJP796vNKSWc0AMANBBEDc8fkN3ffSR5Kkb11SrInDuHYMYBaCCIC4s/5vldp9qEGZKYm6fcZos8sB4hpBBEBccbd49dCrH0uSFs8YpWwWLQNMRRABEFcee2ufjjZ6NDwnTddfUmx2OUDcI4gAiBvuFq/W/OUzSdKdl39JiQ6+AgGz8SkEEDfW/rVCDS1tGpk7QF8fN9jscgCIIAIgTjR7fHpi235J0vemjZDdbjO5IgASQQRAnHj2bwd0tNGjQleK/pkVVIGoQRABYHl+v6Entgd6Q26ZOkIJzA0BogafRgCW9/beI6o81qwMZ4LmTSgwuxwAJyGIALC8Z96pkCTNu7BAKUkOk6sBcDKCCABLO1Tfoj/vrpUUuMIugOhCEAFgaev/Vimf39DEYS6NzE03uxwApyCIALAsn9/Qs387IIneECBaEUQAWNb/7j+q6voWZaYk6v+cxwJmQDQiiACwrBc/qJIkzTpvsJITmKQKRCOCCABL8rT59cqHhySJBcyAKEYQAWBJ2/Ye1vEmr3LSk3Xx8IFmlwOgGwQRAJb04gfVkqQ5X86Xg+vKAFGLIALAcpo9Pr26KzAscwXDMkBUI4gAsJw3PzmsRo9PQ7NSNKEoy+xyAJwGQQSA5by2u0aSdPm4PNlsDMsA0YwgAsBS/H5Dr+0+LEn62pg8k6sBcCYEEQCWsvNgvY6caFVakkMTh7nMLgfAGRBEAFhKxwXupo7OUVICX3FAtONTCsBSOuaHTB+Ta3IlAMJBEAFgGYfqW/ThQbdsNukygggQEwgiACzj9Y8DwzIlhVkaNCDZ5GoAhIMgAsAy3t4TOFtm2mh6Q4BYQRABYAl+v6F39h2TJE0ZxbVlgFhBEAFgCR/XNOhYo0epSQ6dX5BldjkAwkQQAWAJf/n0qCTpn85xKdHBVxsQK/i0ArCEv356RJI0aQTDMkAsIYgAiHltPr/+t31+yKQRg0yuBkBPEEQAxLwPq9xqaG1ThjNB5w7JMLscAD1AEAEQ8/7SPixzyfCBcti52i4QSwgiAGLeX9snqjI/BIg9BBEAMc3nN/R+RZ0kaeIwgggQawgiAGLax4ca1OjxaUBygr40ON3scgD0EEEEQEx770CgN6SkMIv5IUAMIogAiGll7cMyE4qzTa4EwNkgiACIaR09IhcSRICYlNCTjd1utxYvXqz8/HwdO3ZMRUVFKi0t7Xb7119/XZs2bdKQIUP00Ucf6Wtf+5q+853v9LpoAJCkIydaVXG0SVJgaAZA7OlREJk/f74uvfRSrVixQpI0c+ZMOZ1O3X777Z223b17t2688UaVl5fL6XSqra1N48aN06hRozRp0qTIVA8grnWcLTM6b4AyUxJNrgbA2Qh7aGbbtm3asmWLbrjhhmDbwoULtXLlSnm93k7bv/LKK8rMzJTT6ZQkJSQkaPz48XrnnXciUDYAMCwDWEHYQWTTpk1yuVwqKioKtpWUlKiurk5bt27ttH1eXp7+8Y9/aPv27ZIkn8+nsrIyTZ48OQJlA8AXPSITiggiQKwKO4iUlZWpsLAwpK2goCD42KnmzZunMWPG6IorrtDLL7+sH/7wh7r11lt18cUX97JkAJA8bX794/N6SZwxA8SysINITU2N0tLSQtpSUlIkSdXV1Z22T05O1quvvqr8/HzNmTNHbre7y7kkp2ptbZXb7Q65AcCpPqlpUGubXxnOBA0flHbmJwCISmEHEa/XK4fDEdKWkHD6ua4HDx7U+PHjNWfOHD3zzDO68cYbZRjGaZ+zatUqZWZmBm+n9sIAgCTtPBjoDflyQaZsNhYyA2JV2EEkJydHTU1NIW0tLS2SpNzc3E7bV1ZW6qqrrtKvf/1rvfjii1q+fLl++9vf6je/+c1p36e0tFT19fXBW2VlZbglAogjHUHkvKGZJlcCoDfCDiIlJSWqqqoKaautrZUkTZw4sdP2q1ev1oUXXiiXyyWbzab7779fV111ldauXXva90lOTlZGRkbIDQBO9WFHjwhBBIhpYQeRefPmqbq6WjU1NcG28vJyuVwuTZ06tdP2jY2N8vv9IW2XXXbZGYdmAOBMPG1+7a5ukEQQAWJd2EFk+vTpmjVrltasWRNs27hxo5YvX67U1FRJ0qJFizRnzhxJ0ty5c7Vt27Zgr4kkvf/++1qwYEGkagcQpz6paZDHF5ioWuRKNbscAL3Qo5VV161bpyVLlmjZsmVyOp0aMWKEli5dGny8rq4uGDymTJmiJ598Ut///vc1fvx4tbS06Nxzz9Vtt90W2T0AEHeYqApYh82I8rESt9utzMxM1dfXM18EgCRp+aad+t3/HtAtXx2u0lljzS4HQBfC/f3m6rsAYg4TVQHrIIgAiClMVAWshSACIKYwURWwFoIIgJjyUVXgsg/jhjBRFbACggiAmFJ+KBBEzh3C5HXACggiAGJKx/yQMYPTTa4EQCQQRADEDMMwtLu9R2RsPj0igBUQRADEjNqGVtU1eWW3SSNzB5hdDoAIIIgAiBm7DwWGZYbnDJAz0WFyNQAigSACIGbsrg4MyzA/BLAOggiAmNHRI8L8EMA6CCIAYkY5PSKA5RBEAMQET5tfnx4+IUkaQ48IYBkEEQAxYd+RE/L6DKU7EzQk02l2OQAihCACICZ0LGQ2dnAGS7sDFkIQARATOpZ2H5PP/BDASggiAGLCnprA/JBReQQRwEoIIgBiwt7aQBAZzYqqgKUQRABEvWaPT5V1TZJY2h2wGoIIgKj36eETMgzJlZakgQOSzS4HQAQRRABEvY5hGXpDAOshiACIentqA6fujiKIAJZDEAEQ9YJnzBBEAMshiACIeh1DM5y6C1gPQQRAVGtt8+mzo42S6BEBrIggAiCq7T/SKL8hZTgTlJPOGTOA1RBEAES1k1dU5RozgPUQRABEteD8EIZlAEsiiACIaqwhAlgbQQRAVOtYQ4QgAlgTQQRA1PL5DX12NHCNmRE5BBHAiggiAKJW1fFmedr8Skqwa0hWitnlAOgDBBEAUWvfkcD6IecMTJXDzhkzgBURRABErf2HAxNVhw9iWAawKoIIgKjV0SMyLCfN5EoA9BWCCICotb8jiAwiiABWRRABELX2HQ4EkeEEEcCyCCIAolKL16eq+mZJ0nBO3QUsiyACICp9drRRhiFlpiQqOzXR7HIA9BGCCICotP/wF/NDuNgdYF0EEQBRqeOMGeaHANZGEAEQlfYd5owZIB4QRABEpf1H2hczY6IqYGkEEQBRiTVEgPhAEAEQdeoaPapr8kqSzhmUanI1APoSQQRA1OmYqJqf6VRqUoLJ1QDoSwQRAFGnY1hmONeYASyPIAIg6nRMVGV+CGB9BBEAUeeLU3c5YwawOoIIgKizn8XMgLhBEAEQVfx+gzkiQBwhiACIKtXuFrW2+ZXosGloVorZ5QDoYwQRAFGl42J3ha5UJTj4igKsjk85gKhScSwQRM4ZyLAMEA96tFKQ2+3W4sWLlZ+fr2PHjqmoqEilpaVnfF5dXZ1+85vfyDAMjRkzRldddRWX9QbQpQNHmyRJRS5WVAXiQY+CyPz583XppZdqxYoVkqSZM2fK6XTq9ttv7/Y5W7du1R133KFf/vKXmjZtWq+KBWB9Fe1BpHggQQSIB2EPzWzbtk1btmzRDTfcEGxbuHChVq5cKa/X2+Vz3nzzTc2dO1erV68mhAAIS8UxgggQT8IOIps2bZLL5VJRUVGwraSkRHV1ddq6dWun7RsbG7VgwQItXLhQEydOjEy1ACzNMAwdOBqYI1LkYo4IEA/CDiJlZWUqLCwMaSsoKAg+dqrf/va3OnjwoFpbWzV79mwNHjxYc+fO1aFDh3pZMgCrOtroUaPHJ5tNKnRx6i4QD8KeI1JTU6OsrKyQtpSUwBdFdXV1p+03b96s7OxslZaWavjw4aqsrNSll16qefPmafv27d2+T2trq1pbW4P33W53uCUCiHEd80PyM5xKTnCYXA2A/hB2j4jX65XDEfrFkJDQfY6pqKjQV77yFQ0fPlySVFhYqMWLF+svf/lLlz0oHVatWqXMzMzg7dReGADWdaD91N0i5ocAcSPsIJKTk6OmpqaQtpaWFklSbm5up+39fr+SkpJC2iZPnixJ+vTTT7t9n9LSUtXX1wdvlZWV4ZYIIMYFz5hhfggQN8IOIiUlJaqqqgppq62tlaQuJ6OOHj1aBw8eDGnrGNpxuVzdvk9ycrIyMjJCbgDiQ3ANEXpEgLgRdhCZN2+eqqurVVNTE2wrLy+Xy+XS1KlTO21/1VVX6b333tOJEyeCbdXV1RowYIAmTJjQy7IBWBGn7gLxJ+wgMn36dM2aNUtr1qwJtm3cuFHLly9XamrgS2PRokWaM2eOJGnBggWaPHmyHnvsseD269ev15133tlp0isASAzNAPGoRyurrlu3TkuWLNGyZcvkdDo1YsQILV26NPh4XV1dcLjGbrfrj3/8o0pLS7V48WK1tbVp4MCB+slPfhLZPQBgCY2tbTpyInDGHMu7A/HDZhiGYXYRp+N2u5WZman6+nrmiwAWVl7t1qxfvq3MlER9sOJys8sB0Evh/n5z9V0AUYFrzADxiSACICoE1xBhWAaIKwQRAFGBHhEgPhFEAESFA8c4YwaIRwQRAFGhgsXMgLhEEAFgOq/Pr4PHmyUxNAPEG4IIANNVHW+Wz28oKcGuvHSn2eUA6EcEEQCmCw7LuFJlt9tMrgZAfyKIADBd8BoznLoLxB2CCADTHTjavoYI80OAuEMQAWC6A/SIAHGLIALAdF8sZsYaIkC8IYgAMJVhGMEeEYZmgPhDEAFgqiMnPGry+GSzSQXZKWaXA6CfEUQAmKrjYndDMlOUnOAwuRoA/Y0gAsBUJ68hAiD+EEQAmIqr7gLxjSACwFRMVAXiG0EEgKkq2hczK3Zx6i4QjwgiAEwVXMyMHhEgLhFEAJjmRGubjpzwSGJoBohXBBEApjnQPlE1OzVRGc5Ek6sBYAaCCADTdKwhUsTS7kDcIogAME3w1F3WEAHiFkEEgGkqmKgKxD2CCADTHGBVVSDuEUQAmKaifY5IMXNEgLhFEAFgCq/Pr6rjLZIYmgHiGUEEgCkO1jXL5zfkTLQrNz3Z7HIAmIQgAsAUHRNVi1ypstlsJlcDwCwEEQCmONB+jZkirjEDxDWCCABTBNcQYX4IENcIIgBMwRoiACSCCACTsIYIAIkgAsAEhmHoQLBHhDkiQDwjiADod4cbWtXs9cluk4ZmpZhdDgATEUQA9LuO+SFDslKUlMDXEBDP+AYA0O84YwZAB4IIgH7HGiIAOhBEAPQ7Tt0F0IEgAqDfBYdmOHUXiHsEEQD9ruPU3SJ6RIC4RxAB0K8aWrw61uiRxBoiAAgiAPpZx7DMwLQkDUhOMLkaAGYjiADoVweYqArgJAQRAP3qs/ZTdxmWASARRAD0swMsZgbgJAQRAP3qix4RgggAggiAfvbF8u4MzQAgiADoRy1en6rrWySxmBmAAIIIgH5T2X7GTHpyglxpSSZXAyAaEEQA9JvgsMygVNlsNpOrARANCCIA+k1woipX3QXQjiACoN+wmBmAU/VofWW3263FixcrPz9fx44dU1FRkUpLS8N67n/8x39ox44d+u1vf3s2dQKwgM9YQwTAKXrUIzJ//nwVFxfr/vvv13/913/ptdde0y9+8YszPm/fvn26++67z7pIANZQwaqqAE4RdhDZtm2btmzZohtuuCHYtnDhQq1cuVJer7fb5xmGoXvvvVczZ87sXaUAYprX59fBumZJ9IgA+ELYQWTTpk1yuVwqKioKtpWUlKiurk5bt27t9nmPPvqorr32WmVkZPSuUgAxrep4s9r8hpIT7MpLd5pdDoAoEXYQKSsrU2FhYUhbQUFB8LGuVFZW6sMPP9SsWbN6USIAK6g4aX6I3c6puwACwp6sWlNTo6ysrJC2lJQUSVJ1dXWXz1mxYoUeeuihHhXU2tqq1tbW4H23292j5wOITh3zQ4o4dRfAScLuEfF6vXI4HCFtCQnd55gnn3xSV1xxhbKzs3tU0KpVq5SZmRm8ndoLAyA2dZwxcw7zQwCcJOwgkpOTo6amppC2lpbANSNyc3ND2g8dOqR33nlHc+fO7XFBpaWlqq+vD94qKyt7/BoAok8Fp+4C6ELYQaSkpERVVVUhbbW1tZKkiRMnhrRv2bJFjz/+uJxOZ/C2du1arV27Vk6nU2+99Va375OcnKyMjIyQG4DYx6m7ALoS9hyRefPm6ZFHHlFNTY3y8vIkSeXl5XK5XJo6dWrItv/yL/+iiy++OKStY+GzVatWhZx5A8D6/H6DVVUBdCnsIDJ9+nTNmjVLa9as0V133SVJ2rhxo5YvX67U1MAXy6JFi3Tw4EFt3rxZmZmZIc/vuD9mzJhI1Q4gRtQ0tKi1za8Eu01Ds1LMLgdAFOnREu/r1q3TkiVLtGzZMjmdTo0YMUJLly4NPl5XVxccrgGADh3zQ4ZmpyjBwSWuAHyhR0EkMzNTjz/+eLePP/fcc90+xjVmgPjF/BAA3eGfJgD6XPBidy7mhwAIRRAB0Of2Hw70iAwbRI8IgFAEEQB9bv+R9iCSQxABEIogAqBP+f2G9rfPERkxaIDJ1QCINgQRAH2qqr5Znja/Eh02Dc3m1F0AoQgiAPrUvsNfnDHj4Kq7AE5BEAHQp4LzQ5ioCqALBBEAfaojiAxnoiqALhBEAPSpfR1BhB4RAF0giADoU/sOn5AkDeOMGQBdIIgA6DMtXp8OHm+WxBwRAF0jiADoMweONckwpHRnggYNSDK7HABRiCACoM90nLo7fFCabDZO3QXQGUEEQJ/Zd6RjfgjDMgC6RhAB0Ge+uNgdE1UBdI0gAqDPsIYIgDMhiADoM6yqCuBMCCIA+kR9k1dHGz2SCCIAukcQAdAnOiaq5mUkKy05weRqAEQrggiAPhGcH8JEVQCnQRAB0CeC80OYqArgNAgiAPrEyYuZAUB3CCIA+sQ+zpgBEAaCCICI8/sN7W+frDo8hzkiALpHEAEQcQePN6vF61dSgl2F2SlmlwMgihFEAETcntoGSYH5IQkOvmYAdI9vCAARt6cmMCwzKi/d5EoARDuCCICI21PbHkRymR8C4PQIIgAijiACIFwEEQARZRiG9tYE5oiMyiOIADg9ggiAiKqub1Gjx6cEu03FA1lDBMDpEUQARFTHsMywQWlK5IwZAGfAtwSAiNrTPiwzkvkhAMJAEAEQUZ8eZqIqgPARRABEVMcaIiNZQwRAGAgiACLGMAxO3QXQIwQRABFz+ESr6pu9stu46i6A8BBEAETMJ4cCvSHFA9PkTHSYXA2AWEAQARAxuw+5JUlj85kfAiA8BBEAEVNeHTh1d8zgDJMrARArCCIAIqajR2TMYHpEAISHIAIgItp8/uCpu2Pz6REBEB6CCICI2H+kUR6fXwOSEzQ0K8XscgDECIIIgIgoPxSYH/Klwemy220mVwMgVhBEAETE7mrmhwDoOYIIgIjY3d4jMob5IQB6gCACICI6ekTG0iMCoAcIIgB6rb7Jq6r6FknSaIIIgB4giADotY71Q4ZmpSjDmWhyNQBiCUEEQK+VM1EVwFkiiADotZ0HA0Fk3NBMkysBEGsIIgB67cOD9ZKkLxNEAPQQQQRArzR7fNpTGzh1lyACoKcIIgB65aNqt/yGlJOerLyMZLPLARBjCCIAemXn58clBXpDbDaWdgfQMz0KIm63WzfeeKN+/OMf69Zbb9WqVatOu/1bb72lSZMmKT09XRMmTNCf//znXhULIPp0TFQ9j2EZAGchoScbz58/X5deeqlWrFghSZo5c6acTqduv/32Ttu+9957Ki0t1fe+9z01NTXpvvvu0+zZs/XBBx9ozJgxkakegOmYqAqgN8LuEdm2bZu2bNmiG264Idi2cOFCrVy5Ul6vt9P269ev19atW3X99ddr0aJF2rRpkzwej55++unIVA7AdExUBdBbYQeRTZs2yeVyqaioKNhWUlKiuro6bd26tdP2s2fPVmpqavD+P/3TPykrK0t1dXW9LBlAtGCiKoDeCjuIlJWVqbCwMKStoKAg+Nippk2b1qnNMAyVlJSc9n1aW1vldrtDbgCiExNVAfRW2EGkpqZGaWlpIW0pKSmSpOrq6jM+/7333lNKSooWLFhw2u1WrVqlzMzM4O3U8AMgejBRFUBvhR1EvF6vHA5HSFtCQvhzXe+//36tXr06ZLimK6Wlpaqvrw/eKisrw34PAP1rR2VgqHV8AUEEwNkJO0nk5OSoqakppK2lJXDZ79zc3NM+96mnntLEiRP1jW9844zvk5ycrORkxpqBaFfX6NGnhxslSRcUZZtcDYBYFXaPSElJiaqqqkLaamtrJUkTJ07s9nllZWV67733dNddd51liQCiUVl7b8jwQWlypSWZXA2AWBV2EJk3b56qq6tVU1MTbCsvL5fL5dLUqVO7fE51dbVWr16thx56KKR97969Z1kugGjxXkUgiEwopjcEwNkLO4hMnz5ds2bN0po1a4JtGzdu1PLly4PzPhYtWqQ5c+ZIkk6cOKFvfetbuuSSS/T8889rw4YN+v3vf6/vf//7MgwjwrsBoL+9X3FcknQhQQRAL/RoZdV169ZpyZIlWrZsmZxOp0aMGKGlS5cGH6+rqwsO11x99dX685//3GlZ98mTJ2vUqFERKB2AWdp8fu2oPC6JIAKgd2xGlHdPuN1uZWZmqr6+XhkZGWaXA0CBZd2/8attSncm6IOfXi67nTVEAIQK9/ebq+8C6LH3DwTmh1xQlE0IAdArBBEAPdYxUfVCTtsF0EsEEQA99sUZM1nmFgIg5hFEAPTI53VN+ryuWQ67TSWFWWaXAyDGEUQA9MhfPz0qSTq/IFPpzkSTqwEQ6wgiAHqkI4hMGjHQ5EoAWAFBBEDYDMPQX4JBZJDJ1QCwAoIIgLDtP9KoQ+4WJTnsLGQGICIIIgDC9td9gd6QCcVZciY6TK4GgBUQRACErWNY5tLhDMsAiAyCCICw+P2G3umYHzKSiaoAIoMgAiAsH9c06GijRymJDo0vyDK7HAAWQRABEJbXdgeurH3JcJeSEvjqABAZfJsACEtHEPna2DyTKwFgJQQRAGd0rNETvOLu9DG5JlcDwEoIIgDO6I2Pa2UY0rn5GRqSlWJ2OQAshCAC4Iz+HByWoTcEQGQRRACcltfn11sfH5bEsAyAyCOIADitv312TA2tbRqYlsRpuwAijiAC4LS2fHhIknTZmFzZ7TaTqwFgNQQRAN1q8/m1eWe1JGnOl/NNrgaAFRFEAHTrnX3HdOSER1mpiZo8kuvLAIg8ggiAbr34QZUkadZ5+aymCqBP8M0CoEutbT698mFgWOaK8QzLAOgbBBEAXXr7kyNyt7QpNz1ZFw/jarsA+gZBBECXXmgflplzfr4cnC0DoI8QRAB0cqzREzxt919KhppcDQArI4gA6GTDe5Xy+Pw6b2iGzi/INLscABZGEAEQwu83tO7dSknSgouLZbMxLAOg7xBEAIT4676j2n+kUQOSE/TP44eYXQ4AiyOIAAjxzP9WSJLmXjBUackJJlcDwOoIIgCCKo81acuuGknSNy8uMrkaAPGAIAIg6NG39snnNzRl5CCNzc8wuxwAcYAgAkCSVOtu0fq/Byapfv+ykSZXAyBeEEQASJIe27Zfnja/LizO1iXDXWaXAyBOEEQAqK7Ro6ffCUxS/f5lIzhlF0C/IYgA0C//vEdNHp/Ozc/QZV/KNbscAHGEIALEub21DVrb3huyfPZYekMA9CuCCBDn7t9cLp/f0IyxuZoyapDZ5QCIMwQRII69/nGtXv/4sBLsNi2fPdbscgDEIYIIEKfcLV79eONOSdK3Lz1Hw3MGmFwRgHhEEAHi1MoXP1JVfYuKXKlaevlos8sBEKcIIkAc+tNHNXruvc9ls0k/v3o815QBYBqCCBBnDhxt0h0bPpAk3TRlmCYOY/EyAOYhiABxpLG1TTc/9Xcdb/JqfEGmll7+JbNLAhDnCCJAnPD5DS35/Q59XNOgnPRk/c+3LpIz0WF2WQDiHEEEiAM+v6E7n/tAW3bVKMlh139ff6EGZzrNLgsACCKA1fn8hko3/kMbyw7KYbfpP6+7QBcWZ5tdFgBIkpgqD1hYk6dNP3p2h7Z+VCO7TfrltSX6P+cNNrssAAgiiAAWdfB4sxY99XftqnIryWHXw9eM1zfOH2J2WQAQgiACWNALOw7qJ89/qIaWNg1MS9Kj375QFxZzmi6A6EMQASzk87om3b+5XK98eEiSVFKYpV9dd4EKXakmVwYAXSOIABZQ1+jRY9v26bG396u1zS+H3aYfTh+pH1w2UgkO5qQDiF4EESCGfXakUU+/U6HfvXtATR6fJOmS4S6tuGKcxuZnmFwdAJxZj4KI2+3W4sWLlZ+fr2PHjqmoqEilpaXdbv/CCy/oueee0+jRo7Vr1y7dd999+tKXWMkR6I1ad4u2ltfoxQ+q9M6+Y8H2cUMy9MPpo/T1cXmy2WwmVggA4etREJk/f74uvfRSrVixQpI0c+ZMOZ1O3X777Z223bZtm2666Sbt2bNHWVlZ2r59u2bMmKFdu3YpI4N/qQHhOtHaprIDdfr7Z3V645PD+qDyePAxm0366ugcfWfSOZo2OocAAiDm2AzDMMLZcNu2bfrKV76iiooKFRUVSZKeeeYZ/fCHP1RNTY0SExNDtp8xY4aKior0xBNPBNtGjx6tW265RUuXLg27QLfbrczMTNXX1xNgYGktXp8OHm9WxdFGfVJzQp8calD5oQZ9fMgt/ymf0pLCLM08N09zLxiqIVkp5hQMAKcR7u932D0imzZtksvlCoYQSSopKVFdXZ22bt2q2bNnB9vr6ur05ptv6uGHHw55jfHjx2v9+vU9CiJArPH5DTV7fWrytKnZ41NT+62hxau6Jo+ONXpV1+jRsSaPjp3wqLq+WQePN+vICU+3rzk0K0UXnZOti4cN1NfG5iovg+XZAVhD2EGkrKxMhYWFIW0FBQXBx04OIjt37lRbW1uX27/wwgsyDKPbLuTW1la1trYG77vd7nBL7JGHX/1Y7pa2M27XVYdRV11I3fUrGV1s3dW2PXnNrrbu8jW7fJ/wntuTmrp6zTCb2l/z7P8bd71d1+/UZWs3+9PmM+TzG/L6Dfn8fnnb77f5/GrzBx5v84f+3dPmV5PHp9Y2f5fvH460JIcKXakalZeuL+UN0Ki8dI0vyOK6MAAsK+wgUlNTo6ysrJC2lJRAl3B1dXWnbSUpLS2t0/Zer1dHjx7VoEGDunyfVatW6d577w23rLP27N8qVdvQeuYNgbNks0mpiQ6lJCUoNcmhAckJcqUlKTstSa7URGWnJSk7NUl5GU4VZKdoaFaKslITmecBIK6EHUS8Xq8cjtBLhickdP10r9crSWFvf7LS0lItWbIkeN/tdnfqWYmEhZPPUVOrr1N7V78BXf4sdLFhdz8fXb9mF88P97272zbMH7Bw6+lJTb3Zx+50tT/hv3cPXrOLjR12mxLtdjnsNiU4bEqw29v/tCnBYQ/8edJjDrtNSQl2pSQ6lJrkUGpSgpyJdkIFAJxB2EEkJydHTU1NIW0tLS2SpNzc3E7bSupy+8TERGVnd3/lz+TkZCUnJ4db1ln73rSRff4eAADg9MJecrGkpERVVVUhbbW1tZKkiRMnhrSff/75stlsXW4/YcKETj0lAAAgPoUdRObNm6fq6urg/A9JKi8vl8vl0tSpU0O2zcvL05QpU7Rjx46Q9vLycs2dO7d3FQMAAMsIO4hMnz5ds2bN0po1a4JtGzdu1PLly5WaGrig1qJFizRnzhxJ0gMPPKAXXngheNZLRUWFamtrdcstt0SyfgAAEMN6tLLqunXrtGTJEi1btkxOp1MjRowIWROkrq4uOFwzadIkPfLII1q0aJFKSkpUUVGhl19+udOZNwAAIH6FvbKqWVhZFQCA2BPu7zfXBwcAAKYhiAAAANMQRAAAgGkIIgAAwDQEEQAAYBqCCAAAMA1BBAAAmIYgAgAATNOjlVXN0LHeWsdS8QAAIPp1/G6fad3UqA8iDQ0NkqTCwkKTKwEAAD3V0NCgzMzMbh+P+iXe/X6/qqqqlJ6eLpvNFrHXdbvdKiwsVGVlpWWXjrf6Plp9/yTr76PV90+y/j5aff8k6+9jX+2fYRhqaGjQkCFDZLd3PxMk6ntE7Ha7CgoK+uz1MzIyLPk/1smsvo9W3z/J+vto9f2TrL+PVt8/yfr72Bf7d7qekA5MVgUAAKYhiAAAANPEbRBJTk7WihUrlJycbHYpfcbq+2j1/ZOsv49W3z/J+vto9f2TrL+PZu9f1E9WBQAA1hW3PSIAAMB8BBEAAGAagggkSQcOHJDH4zG7DPTA3r17zS4BPdTc3KyDBw+aXQYiiM9h78VlENm0aZPGjx+v9PR0TZkyRWVlZSGPf/TRR7r22mu1cuVKLViwQC+99JJJlfbOZ599pqqqqi4fu/POO2Wz2YK3q6++WklJSf1cYe91t49WOYYd2traNHTo0JBjtnXrVrPLOmtut1s33nijfvzjH+vWW2/VqlWrzC6pT2zevDnkmLlcLqWkpJhdVkRs3769U9tjjz2mG264Qffcc4+++c1v6tChQyZUFhld7V+sfw6bmpp0xx13KD8/X3l5efrXf/1XNTY2Bh837fgZceb55583vv71rxvPPvus8fDDDxtZWVnGwIEDjdraWsMwDKOurs4YPHiw8cYbbxiGYRjHjx83Bg8ebLz77rtmlt0jtbW1xm233WYkJSUZr7/+eqfHm5qajDlz5hhr164N3nbu3Nn/hfbC6fbRCsfwVM8++6yxbNmy4PF65plnjMbGRrPLOmtf//rXjXvuuSd4f8aMGcbDDz9sYkV947rrrgv5nP3pT38yu6Ree/vtt43JkycbxcXFIe2/+93vjNGjRxsej8cwDMN4+umnjQkTJhhtbW0mVHn2uts/w4j9z+H8+fON5cuXGxs3bjQWLlxoSDK+9a1vGYZh7vGLuyBy2223hfyH/f3vf29IMlavXm0YhmGsXLnSGDZsWMhzbrrpJuMb3/hGv9bZG+Xl5cbbb79tSOoyiKxevdr44x//2P+FRdDp9tEKx/BU8+bNM1pbW80uIyI6jltFRUWw7emnnzays7ODX4JW8MknnxhLly41u4yIamhoMD777DPjlltu6fRDPXLkSOPee+8N3m9tbTXS0tKMDRs29HOVZ+90+2cYsf053Llzp/Hf//3fIW2zZ882HA6H0dLSYurxi6uhGY/Ho2uuuUYOhyPYdvnll0uS6urqJAWGbS644IKQ55WUlOjVV1/V8ePH+63W3hgzZsxpl8X/9a9/rblz5+qiiy7Sf/7nf8bk3JDT7aMVjuHJysrK9Ic//EG5ubmaO3eutm3bZnZJvbJp0ya5XC4VFRUF20pKSlRXVxdT3dxn8utf/1oPPfSQRo4cqTvvvFO1tbVml9RrAwYMUHFxsQYPHhzSvnPnTu3duzfkc5eUlKSxY8dq/fr1/V3mWetu/6TY/xx2DIee7PLLL5fP59O7775r6vGLqyCSlJSkSZMmhbT5/X5JgS9Cv9+vDz74oNOVfgsKCuTxeLRr165+q7WvNDY26sorr9TcuXNVXl6uH/3oR5o5c6ZaWlrMLi0irHgMPR6P7rzzTo0ePVrPP/+8pk6dqt/85jdml3XWysrKujw+HY9ZxbnnnqubbrpJjY2N+vnPf64JEyZoz549ZpfVJzqOW1fH1SrHNNY/h5MmTVJiYmJIm9/v15AhQ7R//35J5h2/uAoiXXnttdc0fvx4zZgxQ0ePHpXP51NaWlrINh2Ty6qrq80oMaLS0tL005/+VM8995wOHDigb37zm3rrrbf0b//2b2aXFhFWPIYXX3yxHnzwQb377rvavn27zjnnHP3oRz/SJ598YnZpZ6WmpsZSx6c7t9xyi1avXq3Kyko9+OCDqq6u1ne/+12zy+oTNTU1ktTlcbXKMbXa51AK/P7dcccdph+/uA4ifr9fDz/8sJ544gnZbDZ5vV5JChm6kaSEhKi/SPFZGThwoJ5++mlNmzZNzzzzjNnlRITVj+GkSZO0ZcsW2Wy2mOryPpnX67Xs8elKQkKC7rzzTv3kJz/R22+/rQMHDphdUsRZ/XN3Kit8Dt9//30dP35cP/jBD0w/fpYKIvfcc0/IaVWn3k4d97v//vv1gx/8QBMmTJAkuVwu2e12NTU1hWzXMWyRm5vbPzvSjZ7uXzhsNpu++93vqrKysg8q7rne7mO0H8NTnc3+jho1Sl/96lej5pj1VE5OTswcn0i6+eabJUmff/65yZVEXk5OjiR1eVytekxj+XPY3Nysu+++Wxs2bFBiYqLpx89ScfW2227T9ddf3+3jJ6e7V155RYmJibruuuuCbU6nU2PGjOm0LkVtba0SEhKCgcUsPdm/nsjOztbQoUPPtqyI6u0+RvsxPNXZ7m80HbOeKikp0aZNm0LaOiZyTpw40YyS+kV2drYkxexxO52SkhJJUlVVlc4777xge21treWPaSwez6VLl+rBBx9UXl6eJPOPn6WCiMvlksvlOuN2u3bt0htvvKF///d/D7Z5vV5VVVVp3rx52rBhQ8j25eXlmjFjhgYMGBDxmnsi3P3rqbKyMs2fPz/ir3s2IrGP0XwMT3U2+2sYhnbu3Kl77rmnb4rqY/PmzdMjjzyimpqa4BdheXm5XC6Xpk6danJ1faesrEwTJ05UcXGx2aVE3EUXXaTi4mLt2LEjeCaiJO3evVu33nqriZX1nVj9HP7sZz/TNddco3HjxgXbsrKyTD1+lhqaCUd1dbW+973v6cILL9SGDRu0YcMGPfvss7r55puVnZ2t22+/XfX19XrnnXckBVbSe+WVV7RixQqTK+8Zn88nKfBhOdmjjz6qm2++Odhj8Omnn+rtt9/W3Xff3e819lZ3+2iVYygF5jFdc801+tWvfhU8w+tXv/qVvv3tb4d8kcSS6dOna9asWVqzZk2wbePGjVq+fLlSU1NNrCxy/vrXv2revHn629/+JinQ5f2zn/1Mjz/+uMmVRYbP5wv53NlsNj344IN66qmn1NbWJimwMml+fr6uuuoqs8o8a6fun1U+h7/73e+0c+dOHT58OPj79/jjj2vt2rWmHj9L9YicSUtLi2bPnq0dO3borbfeCnlswYIFysjIkCT96U9/0t13363zzz9fx44d0/33369LLrnEjJLPyvvvv6+nnnpKkrR27VolJSVp8uTJkgJdiS+++KI2bdqka6+9VkOHDtUf/vCHqOspOJMz7WOsH8MOdrtdDodDy5Yt01NPPaXLLrtMl1xyia688kqzS+uVdevWacmSJVq2bJmcTqdGjBihpUuXml1WxGRlZam8vFxTpkzR1VdfreLiYv3iF7/Q8OHDzS6tVzwej1599VW99NJLqq6u1pNPPqlp06apuLhY8+fPV319vW688UaNGzdOFRUV2rx5c6cJkNHsdPsX65/Dt956SzfccIM8Ho/Wrl0b8tjWrVs1Y8YM046fzTj1n5MAAAD9JO6GZgAAQPQgiAAAANMQRAAAgGkIIgAAwDQEEQAAYBqCCAAAMA1BBAAAmIYgAgAATEMQAQAApiGIAAAA0xBEAACAaQgiAADANAQRAABgmv8fngjMtzBLgWUAAAAASUVORK5CYII=\n",
    "\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Likelihood with negative sampling\n",
    "<div align='center'><img src=\"figs/word2vec_negative_onesample.png\" width='45%' ></div>\n",
    "<br>\n",
    "$$ \\max_{\\params} \\underbrace{p(y=+1|w_{t-1},w_t;\\params)}_{\\text{pos. center vs context}}\\cdot \\underbrace{\\prod_{k=1}^K p(y=0|w_k,w_t;\\params)}_{\\text{neg. center vs k samples}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Likelihood with negative sampling\n",
    "\n",
    "$$ \\max_{\\params} \\underbrace{p(y=+1|w_{t-1},w_t;\\params)}_{\\text{pos. center vs context}}\\cdot \\underbrace{\\prod_{k=1}^K p(y=0|w_k,w_t;\\params)}_{\\text{neg. center vs k samples}}$$\n",
    "\n",
    "We can apply $\\log$ since is strictly monotonic, will not change the the optimization:\n",
    "\n",
    "$$ \\max_{\\params} \\log p(y=+1|w_{t-1},w_t;\\params)+ \\log \\sum_{k=1}^K p(y=0|w_k,w_t;\\params)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\max_{\\params} \\log p(y=+1|w_{t-1},w_t;\\params)+ \\log \\sum_{k=1}^K \\underbrace{1- p(y=+1|w_k,w_t;\\params)}_{\\text{it is a binary classifier!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Likelihood with negative sampling\n",
    "\n",
    "$$ \\max_{\\params} \\log p(y=+1|w_{t-1},w_t;\\params)+  \\sum_{k=1}^K \\log\\big[1- p(y=+1|w_k,w_t;\\params)\\big]$$\n",
    "\n",
    "We replace $p(\\cdot)$ with the logistic regression and using $\\sigma(-z)= 1-\\sigma(z)$, we get:\n",
    "\n",
    "$$ \\max_{\\params} \\log \\sigma \\left(\\params_{C}[gt]^T\\params_W[i]\\right)+ \\sum_{k=1}^K \\log \\big[ \\sigma\\left(-\\params_{C}[k]^T\\params_W[i]\\right)\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss with negative sampling\n",
    "\n",
    "If we minimize, we have to invert the sign:\n",
    "\n",
    "$$ \\min_{\\params} -\\log \\sigma \\left(\\params_{C}[gt]^T\\params_W[i]\\right) - \\sum_{k=1}^K \\log \\big[ \\sigma\\left(-\\params_{C}[k]^T\\params_W[i]\\right)\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualization\n",
    "\n",
    "\n",
    "<div align='center'><img src=\"figs/negative_params.png\" width='65%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to sample the negatives\n",
    "\n",
    "Which is, how to select the indexes $k \\in \\{1\\ldots K\\}, k \\neq gt$ in $\\sum_{k=1}^K \\log \\big[ \\sigma\\left(-\\params_{C}[k]^T\\params_W[i]\\right)\\big]$ ?\n",
    "\n",
    "We sample from an **Unigram model** defined over the corpus $V$ as:\n",
    "\n",
    "$$ P(v)_{\\alpha} = \\frac{\\operatorname{count(v)^\\alpha}}{\\sum_{v^{\\prime}}\\operatorname{count(v^{\\prime})^\\alpha}}$$\n",
    "and fixing $\\alpha=\\frac{3}{4}=0.75$\n",
    "\n",
    "<br>\n",
    "Setting $\\alpha=0.75$ gives better performance because <b>gives rare noise words slightly\n",
    "higher probability</b>for rare words, $P(v)_{\\alpha} > P(v)$ while <b>common words are more or less kept the same.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to sample negative\n",
    "\n",
    "$$ P(v)_{\\alpha} = \\frac{\\operatorname{count(v)^\\alpha}}{\\sum_{v^{\\prime}}\\operatorname{count(v^{\\prime})^\\alpha}}$$\n",
    "and fixing $\\alpha=\\frac{3}{4}=0.75$\n",
    "<br><br>\n",
    "**Example:**\n",
    "$$\\text{is:} \\quad {0.9}^{.75} = 0.92\\\\\n",
    "\\text{Constitution:} \\quad {0.09}^{.75} = 0.16\\\\\n",
    "\\text{bombastic:} \\quad 0.01^{.75} = 0.032$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sparse Gradients\n",
    "\n",
    "- We iteratively take gradients at each window for SGD\n",
    "- In each window, we only have at most $2m + 1$ words plus $2km$ negative words with negative sampling, so the gradient over a window $\\nabla_{\\params}\\mathcal{L}_t({\\params})$ is very sparse!\n",
    "- Computationally, it is important to not have to send gigantic updates around.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta)=\\left[\\begin{array}{l}\n",
    "\\mbf{0} \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_{\\params_W{_{l i k e}}} \\\\\n",
    "\\vdots \\\\\n",
    "\\mbf{0} \\\\\n",
    "\\nabla_{\\params_W{_{am}}} \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_{\\params_W{_{learning}}} \\\\\n",
    "\\vdots\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{2 d V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling word2vec with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Softmax in NLP\n",
    "\n",
    "It is an alternative to **Negative Sampling.** We do not use a contrastive method yet we approximate the normalization over the large vocabulary with a **balanced binary tree structure**.\n",
    "\n",
    "Computational cost reduces from $\\mathcal{O} \\big( |V| \\big)$ to $\\mathcal{O}\\big(\\log_2(|V|)\\big)$ **in the best case.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regular Softmax is a degenerate tree!\n",
    "\n",
    "We change point of view: softmax is a tree of depth=1 and $|V|$ children that are leaf too!\n",
    "\n",
    "Corpus `this is an example of a huffman tree` and assume <ins>**word tokens are characters** to simplify.</ins>\n",
    "<br><br>\n",
    "<div align='center'><img src=\"figs/softmax_tree_linear.png\" width='45%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Hierarchical Softmax in NLP\n",
    "\n",
    "1. Given a vocabulary of word token $V$ how to construct the tree (there are multiple ways of doing it)\n",
    "2. How to train with the tree\n",
    "3. How to perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Huffman tree\n",
    "\n",
    "Given a vocabulary of word token $V$ how to construct the tree? We use **Huffman trees**\n",
    "\n",
    "The corpus is `this is an example of a huffman tree` and assume <ins>**word tokens are characters** to simplify.</ins>\n",
    "\n",
    "| **chars** | **e** | **a** | **n** | **t** | **m** | **o** | **u** | **i** | **h** | **s** | **x** | **p** | **' '** | **f** | **r** | **l** |\n",
    "|-----------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|---------|-------|-------|-------|\n",
    "| **freq**  | 4     | 4     | 2     | 2     | 2     | 1     | 1     | 2     | 2     | 2     | 1     | 1     | 7       | 3     | 1     | 1     |\n",
    "\n",
    "<div align='center'><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Huffman_tree_2.svg/938px-Huffman_tree_2.svg.png\" width='35%' ></div>\n",
    "\n",
    "<small>picture from Wikipedia</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Huffman tree\n",
    "\n",
    "- vocabulary $V \\rightarrow$ word frequencies with **Unigram model.**\n",
    "- More frequent word tokens are placed closer to the root; rare words are at deeper layers.\n",
    "    - If you think Information Theory, we \"spend\" less in encoding frequent words and each word has a variable code length.\n",
    "    - We want to encode `'e'` with a bit string. Convention: left is `0` and right is `1`\n",
    "    -  Then encoding of `'e'` is `left->left->left` which is `000` 3 bits\n",
    "    - Yet the encoding of `'p'` is `right->left->left->right->right` which is `10011` 5 bits\n",
    "- Each node has always two children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Huffman tree\n",
    "\n",
    "| **chars** | **e** | **a** | **n** | **t** | **m** | **o** | **u** | **i** | **h** | **s** | **x** | **p** | **' '** | **f** | **r** | **l** |\n",
    "|-----------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|---------|-------|-------|-------|\n",
    "| **freq**  | 4     | 4     | 2     | 2     | 2     | 1     | 1     | 2     | 2     | 2     | 1     | 1     | 7       | 3     | 1     | 1     |\n",
    "\n",
    "<br><br>\n",
    "<div align='center'><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Huffman_tree_2.svg/938px-Huffman_tree_2.svg.png\" width='55%' ></div>\n",
    "\n",
    "<small>picture from Wikipedia</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training with Hierarchical Softmax\n",
    "\n",
    "We do not model anymore $\\params$ as the number of words in $V$, yet we model $\\params$ as the number of internal nodes in the tree $V-1$.\n",
    "\n",
    "We have a feature vector to be learned at each node $i$ of the tree for a total of $2V-1$ vectors to be learned (context and center).\n",
    "\n",
    "Assume Skip-gram and we want to compute $p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language.\n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_1.png\" width='35%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training with Hierarchical Softmax\n",
    "\n",
    "Assume Skip-gram and we want to compute $p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "So we \"bypass\" all path excepts the one that from root leads to `language`.\n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_2.png\" width='35%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "$$p(w_{t-1}|w_t = \\text{natural}) = \\prod_{n~\\in~path(\\text{root}\\rightarrow \\text{language})} p_{\\operatorname{branch}}\\big(n,i\\big)$$\n",
    "\n",
    "where: \n",
    "$$ \n",
    "p_{\\operatorname{branch}}(n,i) = \\begin{cases} \\sigma\\big(\\boldsymbol{\\theta}_C[n]^T\\boldsymbol{\\theta}_W[i])\\big), & \\mbox{if } \\mbox{left} \\\\ \n",
    "1-\\sigma\\big(\\boldsymbol{\\theta}_C[n]^T\\boldsymbol{\\theta}_W[i])\\big), & \\mbox{if } \\mbox{right} \\end{cases} \n",
    "$$\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hsoftmax_3.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3a.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/hsoftmax_3b.png?2\" width='75%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Embeddings as a Matrix\n",
    "\n",
    "The embedding $\\params_C$ are still in a matrix, where each row of the matrix is indexed by the node $n$ index.\n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_matrix.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3b.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3c.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3d.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3e.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3f.png?2\" width='45%' ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural})$ and we assume the ground-truth $w_{t-1}$ is language. \n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3g.png?2\" width='45%' ></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: compute the loss with Hierarchical Softmax\n",
    "\n",
    "$p(w_{t-1}|w_t = \\text{natural}) = 1 \\cdot 0.9\\cdot0.95\\cdot0.65\\cdot0.2 = 0.11115$\n",
    "\n",
    "Loss is $-\\log\\big(p(w_{t-1}|w_t = \\text{natural})\\big) = -\\log(0.11115)$\n",
    "\n",
    "<div align='center'><img src=\"figs/hsoftmax_3g.png?2\" width='45%' ></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inference with Hierarchical Softmax\n",
    "\n",
    "**Important:** In inference with do not have the label!\n",
    "\n",
    "1. Exhaustive search [too complex]\n",
    "2. Greedy search (at each branch take the branch at maximum probability) [too greedy]\n",
    "3. **Beam search** (we will cover later on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Allo project used Hierarchical Softmax\n",
    "Google (now dead) project Allo used a Hierarchical to speed up inference.\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/allo_2016.png\" width='35%' ></div>\n",
    "\n",
    "<small>Taken from https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Allo project used Hierarchical Softmax\n",
    "\n",
    "Google (now dead ‚ò†Ô∏è ) project Allo used a Hierarchical tree to speed up inference.\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/allo_2016_beam_search.png\" width='45%' ></div>\n",
    "\n",
    "<small>Taken from https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Allo project used Hierarchical Softmax\n",
    "\n",
    "> As with any large-scale product, there were several engineering challenges we had to solve in generating a set of high-quality responses efficiently. For example, in spite of the two staged architecture, our first few networks were very slow and required about half a second to generate a response. This was obviously a deal breaker when we are talking about real time communication apps! \n",
    "\n",
    "> So we had to evolve our neural network architecture further to reduce the latency to less than 200ms. We moved from using **a softmax layer to a hierarchical softmax layer which traverses a tree of words instead of traversing a list of words thus making it more efficient.**\n",
    "\n",
    "<small>Taken from https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating Word Embeddings\n",
    "\n",
    "Related to general evaluation in NLP/Machine Learning: **Intrinsic vs. Extrinsic**\n",
    "\n",
    "- **Intrinsic**:\n",
    "\t- Evaluation on a specific/intermediate subtask\n",
    "\t- Fast to compute\n",
    "\t- Helps to understand that system\n",
    "\t- Not clear if really helpful unless correlation to real task is established\n",
    "    - Often involve **correlation with human judgments**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Extrinsic**:\n",
    "\t- Evaluation on a real downstream task\n",
    "\t- Can take a long time to compute accuracy\n",
    "\t- Unclear if the subsystem is the problem or its interaction or other subsystems\n",
    "\t- If replacing exactly one subsystem with another improves accuracy $\\rightarrow$ Winner! **(Ablation study)**\n",
    "\t- Always perform validation of the hyper-parameter on a validation (or dev) set; when you are sure then test once.\n",
    "\n",
    "<small>Taken from cs224n Stanford NLP with Deep learning</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedding and Historical Semantics\n",
    "<br>\n",
    "<div align='center'><img src=\"figs/historical.png\" width='65%' ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias and Embeddings ‚ö†Ô∏è\n",
    "\n",
    "In addition to their ability to learn word meaning from text, embeddings, alas, also **reproduce the implicit biases and stereotypes** that were **latent in the text**\n",
    "\n",
    "Besides the notorious $$ \\text{man} : \\text{king} = \\text{woman} : \\text{queen}  $$\n",
    "\n",
    "the same embeddings analogies also **exhibit gender bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{man} : \\text{computer programmer} = \\text{woman} : \\text{homemaker}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{father} : \\text{doctor} = \\text{mother} : \\text{nurse}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias and Embeddings ‚ö†Ô∏è\n",
    "\n",
    "Bias in the embeddings can cause **allocational harm**\n",
    "\n",
    ">when a system allocates resources (jobs or credit) unfairly to different groups. For example algorithms\n",
    "that use embeddings as part of a search for hiring potential programmers or doctors\n",
    "might thus incorrectly downweight documents with women's names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias and Embeddings ‚ö†Ô∏è\n",
    "\n",
    "Bias in the embeddings can cause **representational harm** as in bias towards the ethnicity groups\n",
    "\n",
    "> Using such methods, people\n",
    "in the United States have been shown to associate African-American names with\n",
    "unpleasant words (more than European-American names), male names more with\n",
    "mathematics and female names with the arts, and old people‚Äôs names with unpleasant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Debiasing in NLP/Machine Learning is an Open-Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word Embedding: Topics we did NOT cover\n",
    "\n",
    "- **Global Vector - GloVe** Model: taking the best of both word (SVD-based and iterative, word2vec based). Invented by Stanford.\n",
    "- **[fastext](https://fasttext.cc/)**: extension of word2vec to deal with the **Out of Vocabulary (OOV) problem**\n",
    "    - [Fastext demo with Gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='myheader'>Natural Language Processing<img src='../sapienza_logo.png'/></div>",
   "transition": "linear"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Summary",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1143px",
    "top": "173px",
    "width": "210.344px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
