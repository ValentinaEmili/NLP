{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part_1_7_Language Models**\n",
    "\n",
    "In the evolving field of Natural Language Processing (NLP), **Language Models** are foundational in generating, understanding, and analyzing text. From classic statistical models like `n-grams` to advanced Large Language Models (`LLMs`) like those powering modern conversational AI, language models shape applications across search engines, chatbots, summarization, translation, and more. This tutorial covers two main aspects of language models: **n-grams** and **Large Language Models (LLMs)**, along with hands-on exercises using the the **ChatGPT API** and **LangChain** to build sophisticated prompt engineering workflows.\n",
    "\n",
    "### **Objectives:**\n",
    "\n",
    "By the end of this notebook, Parham will:\n",
    "1. Gain an understanding of **n-gram language models**, their structure, and their role in basic text generation and probability-based language modeling.\n",
    "2. Develop a fundamental knowledge of **Large Language Models (LLMs)** and their powerful role in NLP, including how they interpret and generate text in a human-like manner.\n",
    "3. Learn to interact with an LLM, specifically through the **ChatGPT API** and **LangChain**.\n",
    "4. Experiment with **prompt engineering** techniques to customize language model outputs, creating contextually relevant and refined responses.\n",
    "\n",
    "### **Table of Contents:**\n",
    "1. Import Libraries\n",
    "2. Introduction to Language Modeling\n",
    "3. N-gram Models\n",
    "   <!-- - Overview and Theory\n",
    "   - Implementing Unigram, Bigram, and Trigram Models\n",
    "   - Probability and Smoothing Techniques\n",
    "   - Applications and Limitations of N-gram Models -->\n",
    "4. Using ChatGPT API for Language Modeling\n",
    "   <!-- - Introduction to OpenAI’s ChatGPT API\n",
    "   - Basic Setup and Request Handling\n",
    "   - Generating Text and Answering Questions -->\n",
    "5. LangChain and Prompt Engineering\n",
    "   <!-- - Overview of LangChain for Building Applications with LLMs\n",
    "   - Basics of Prompt Engineering: Designing Effective Prompts\n",
    "   - Experimenting with Prompt Variations to Improve Model Responses\n",
    "   - Use Cases: Building a Question-Answering Bot, Text Summarizer, or Conversational Agent -->\n",
    "6. Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import spacy\n",
    "from loguru import logger\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import words\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling is the way of determining the probability of any sequence of words. Language modeling is used in various applications such as Speech Recognition, Spam filtering, etc. Language modeling is the key aim behind implementing many state-of-the-art Natural Language Processing models.\n",
    "\n",
    "### Methods of Language Modelling\n",
    "Two methods of Language Modeling:\n",
    "\n",
    "- **Statistical Language Modelling**: Statistical Language Modeling, or Language Modeling, is the development of probabilistic models that can predict the next word in the sequence given the words that precede. Examples such as N-gram language modeling.\n",
    "\n",
    "- **Neural Language Modeling**: Neural network methods are achieving better results than classical methods both on standalone language models and when models are incorporated into larger models on challenging tasks like speech recognition and machine translation. A way of performing a neural language model is through word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview and Theory\n",
    "\n",
    "An **N-gram** is a contiguous sequence of \\( n \\) items from a given text or speech sample, where the items can be letters, words, or even base pairs depending on the application. Typically, N-grams are extracted from a large corpus, providing insights into text patterns and dependencies.\n",
    "\n",
    "For instance, N-grams can be:\n",
    "- **Unigrams**: Individual words like “This,” “article,” “is,” “on,” and “NLP.”\n",
    "- **Bigrams**: Word pairs like “This article,” “article is,” “is on,” and “on NLP.”\n",
    "\n",
    "An **N-gram language model** estimates the likelihood of a word given a specific context or history. For example, a bigram model estimates the probability of each word given the previous word. The model's goal is to predict the next word, capturing dependencies and patterns in language sequences.\n",
    "\n",
    "**Calculating N-gram Probabilities**\n",
    "\n",
    "For example, in the sentence **“This article is on...”**, if we want to predict the probability that the next word is “NLP,” this can be represented as:\n",
    "$$p(\\text{“NLP”} | \\text{“This”}, \\text{“article”}, \\text{“is”}, \\text{“on”})$$\n",
    "This probability is part of a conditional probability chain that models the probability of each word in a sentence based on its predecessors.\n",
    "\n",
    "To generalize, the conditional probability of the \\(n\\)-th word given the preceding \\(n-1\\) words can be written as:\n",
    "$$P(W) = p(w_n | w_1, w_2, ..., w_{n-1})$$\n",
    "Using the **chain rule of probability**, this probability of a word sequence \\( w_1, w_2, ..., w_n \\) can be expanded as:\n",
    "$$P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, ..., w_{i-1})$$\n",
    "\n",
    "**Markov Assumptions and Simplified Models:**\n",
    "\n",
    "In practice, language models often simplify this calculation by applying **Markov assumptions**. This assumption posits that the probability of a word depends only on a limited history of previous words, rather than the entire sequence. Specifically, in an \\(k\\)-gram model, we assume that each word depends only on the previous \\(k\\) words.\n",
    "\n",
    "- For a **unigram model** (where \\(k = 0\\)), each word is considered independently:\n",
    "  $$\n",
    "  P(w_1, w_2, ..., w_n) \\approx \\prod_{i=1}^{n} P(w_i)\n",
    "  $$\n",
    "\n",
    "- For a **bigram model** (where \\(k = 1\\)), each word depends only on the immediately preceding word:\n",
    "  $$\n",
    "  P(w_i | w_1, w_2, ..., w_{i-1}) \\approx P(w_i | w_{i-1})\n",
    "  $$\n",
    "\n",
    "By applying these assumptions, we make the model computationally feasible while still capturing relevant language patterns. This approach allows us to approximate word dependencies and make educated predictions, forming the basis of applications like autocomplete and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using ChatGPT API for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangChain and Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Closing Thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
